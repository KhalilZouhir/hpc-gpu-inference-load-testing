## Gemma 27b-it (Without Load Balancer) - HPC Deployment & Monitoring

This guide provides the workflow for deploying Gemma 27b-it on HPC using vLLM, setting up monitoring, and running local performance stress tests.

---

## 1. HPC Setup & Monitoring
First, prepare the environment on the HPC cluster to track hardware metrics during the tests.

**Step 1: Create Log Directory**
Inside the hpc , create a directory to collect hardware logs:
`mkdir gpu_tests_log`

**Step 2: Monitoring Script**
Place the script `calcul_cpu_ram_gpu_%_per_30s.sh` inside the folder. This script monitors:
* CPU Usage %
* Memory Usage %
* GPU Util %
* VRAM Usage %

**Step 3: Launch the LLM**
Submit the job using the PBS script:
`qsub vllm_gemma_&_monitoring.pbs`

*Note: For full details on vLLM HPC deployment, see: https://github.com/KhalilZouhir/vllm-hpc-deployment-guide. Ensure you adapt paths, ports, and names to your setup.*

---

## 2. Connectivity & Verification

**Step 1: Check Job Status**
Verify the job is running and identify the node (e.g., gpu08):
`qstat -f JOBID | grep exec_host`

**Step 2: Internal Curl Test**
Test the LLM directly from the login node:
`curl http://gpu08:9996/v1/completions -H "Content-Type: application/json" -d '{"model":"/home/skiredj.abderrahman/models/gemma27b-it-int4-awq","prompt":"do u have a wish","max_tokens":50}'`

**Step 3: SSH Tunneling**
Establish a tunnel from your local machine:
`ssh -N -L 127.0.0.1:9998:gpu08:9998 skiredj.abderrahman@172.30.30.11`

**Step 4: Local Test**
Test the tunnel in a new terminal:
`curl -X POST http://localhost:9998/v1/completions -H "Content-Type: application/json" -d "{\"model\":\"/home/skiredj.abderrahman/models/gemma27b-it-int4-awq\",\"prompt\":\"do u have a wish\",\"max_tokens\":50}"`

---

## 3. Visualization Stack (Docker)

Assuming you have cloned the repo, navigate to the folder and start the services:

`cd .\gemma-27b-it-int4-awq\without_load_balancer`
`docker-compose up -d`

**Wait 30 seconds for services to start, then verify:**
`Start-Sleep -Seconds 30`
`docker ps`

**Access Grafana:**
* URL: http://localhost:3000/
* Login/Password: admin / admin
* Import Dashboards from: http://localhost:3000/dashboards
* Required JSONs: `Load_testing_results.json` & `tokens_testing_results.json`

---

## 4. Running Performance Tests (K6)

**Step 1: Reset InfluxDB**
Clear old data to start fresh:
`docker exec influxdb influx -execute "DROP DATABASE k6"`
`docker exec influxdb influx -execute "CREATE DATABASE k6"`

**Step 2: Execute K6 Test**
update vllm-test.js script and adapt it to your need 
Open PowerShell in the `without_load_balancer` folder and run:
`$currentDir = (Get-Location).Path`
`docker run --rm -i --network host -v "${currentDir}/vllm-test.js:/vllm-test.js" grafana/k6:latest run --out influxdb=http://localhost:8086/k6 /vllm-test.js 2>&1 | Tee-Object -FilePath k6_output.txt`

---

## 5. Data Collection
1. **K6 Logs:** Collect `k6_output.txt` once the test finishes.
2. **HPC Logs:** Run the `calcul_cpu_ram_gpu_%_per_30s.sh` on the HPC and check `gpu_tests_log/1gpu_output.log` for KV cache usage.
3. **Screenshots:** Use the "Go Full Page" extension in Chrome to capture the 2 Grafana dashboards.

*Tip: Visualizing logs is easier if you run `vllm serve` directly in the GPU node instead of as a background script.*
